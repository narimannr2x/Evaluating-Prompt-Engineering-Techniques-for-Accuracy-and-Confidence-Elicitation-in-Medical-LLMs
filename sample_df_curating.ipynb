{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_path = r'c:\\Users\\Rayan\\Desktop\\extraction pipeline\\token_exctracted_jsonl\\board.jsonl'\n",
    "df = pd.read_json(board_path, lines=True, orient='records', encoding='utf-8')\n",
    "df.head()\n",
    "\n",
    "\n",
    "def filter_non_image_questions(df):\n",
    "    \"\"\"\n",
    "    Filter DataFrame to include only questions without images in stem or options.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame containing exam questions\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame containing only non-image questions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create mask for questions without images\n",
    "        no_image_mask = df['image'].apply(\n",
    "            lambda x: (\n",
    "                x.get('has_image_stem', True) == False and  # Using True as default to exclude if key missing\n",
    "                x.get('options_has_pic', True) == False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Apply filter\n",
    "        filtered_df = df[no_image_mask].copy()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        total_questions = len(df)\n",
    "        filtered_questions = len(filtered_df)\n",
    "        \n",
    "        print(f\"Original number of questions: {total_questions}\")\n",
    "        print(f\"Questions without images: {filtered_questions}\")\n",
    "        print(f\"Percentage of non-image questions: {(filtered_questions/total_questions)*100:.2f}%\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while filtering: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is called 'df':\n",
    "non_image_df = filter_non_image_questions(df)\n",
    "print(len(non_image_df))\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def stratified_sample_exam_questions(df, n_samples=500):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling on exam questions based on exam_type and exam_topic combinations.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame containing the exam questions\n",
    "    n_samples (int): Number of samples desired in the final dataset\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Stratified sample of the input DataFrame\n",
    "    \"\"\"\n",
    "    # Create a combined stratum from exam_type and exam_topic\n",
    "    df['stratum'] = df['metadata'].apply(lambda x: f\"{x['exam_type']}_{x['exam_topic']}\")\n",
    "    \n",
    "    # Get the frequency of each stratum\n",
    "    stratum_counts = df['stratum'].value_counts()\n",
    "    \n",
    "    # Calculate the proportion of samples to take from each stratum\n",
    "    total_samples = len(df)\n",
    "    proportions = stratum_counts / total_samples\n",
    "    \n",
    "    # Calculate number of samples to take from each stratum\n",
    "    # Using ceil to ensure we get at least 1 sample from each stratum\n",
    "    samples_per_stratum = np.ceil(proportions * n_samples).astype(int)\n",
    "    \n",
    "    # Adjust if we're taking too many samples\n",
    "    while samples_per_stratum.sum() > n_samples:\n",
    "        # Find the stratum with the most samples and reduce it by 1\n",
    "        max_stratum = samples_per_stratum.idxmax()\n",
    "        samples_per_stratum[max_stratum] -= 1\n",
    "    \n",
    "    # Sample from each stratum\n",
    "    sampled_dfs = []\n",
    "    for stratum, n in samples_per_stratum.items():\n",
    "        stratum_df = df[df['stratum'] == stratum]\n",
    "        # If we need more samples than available in the stratum, take all available\n",
    "        n = min(n, len(stratum_df))\n",
    "        sampled_dfs.append(stratum_df.sample(n=n, random_state=42))\n",
    "    \n",
    "    # Combine all sampled DataFrames\n",
    "    final_sample = pd.concat(sampled_dfs)\n",
    "    \n",
    "    # Drop the temporary stratum column\n",
    "    final_sample = final_sample.drop('stratum', axis=1)\n",
    "    \n",
    "    # Shuffle the final sample\n",
    "    final_sample = final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# Example usage:\n",
    "# df = your_dataframe  # Your original DataFrame\n",
    "\n",
    "\n",
    "# To verify the distribution:\n",
    "def print_distribution(df):\n",
    "    \"\"\"\n",
    "    Print the distribution of exam_type and exam_topic combinations in the DataFrame.\n",
    "    \"\"\"\n",
    "    counts = df.groupby(\n",
    "        df['metadata'].apply(lambda x: (x['exam_type'], x['exam_topic']))\n",
    "    ).size()\n",
    "    \n",
    "    print(\"Distribution of samples:\")\n",
    "    for (exam_type, exam_topic), count in counts.items():\n",
    "        print(f\"Exam Type: {exam_type}, Topic: {exam_topic}, Count: {count}\")\n",
    "\n",
    "\n",
    "sampled_df = stratified_sample_exam_questions(non_image_df, n_samples=500)\n",
    "print_distribution(sampled_df)\n",
    "\n",
    "\n",
    "sampled_df.to_json('sampled_board.jsonl', orient='records', lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
